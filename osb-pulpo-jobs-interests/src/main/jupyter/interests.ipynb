{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import math\n",
    "import os.path\n",
    "import requests\n",
    "import sys, os, unicodedata\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "\n",
    "from boto3 import Session\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.io.json import json_normalize\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCS Messaging configurations\n",
    "EUREKA_SERVER_HOST = os.environ.get(\"EUREKA_SERVER_HOST\", default=\"localhost\")\n",
    "EUREKA_SERVER_PORT = os.environ.get(\"EUREKA_SERVER_PORT\", default=8761)\n",
    "\n",
    "LCS_MESSAGING_ENABLED = bool(os.environ.get(\"LCS_MESSAGING_ENABLED\", default=False))\n",
    "\n",
    "INDIVIDUAL_LCS_DESTINATION_NAME = os.environ.get(\"INDIVIDUAL_LCS_DESTINATION_NAME\", \"interests_pulpo/individual_interests_chunk_add_pre\")\n",
    "SEGMENT_LCS_DESTINATION_NAME = os.environ.get(\"SEGMENT_LCS_DESTINATION_NAME\", \"interests_pulpo/individual_segment_interests_chunk_add_pre\")\n",
    "\n",
    "os.write(1, \"\\nEureka server host: {}:{}\".format(EUREKA_SERVER_HOST, EUREKA_SERVER_PORT).encode())\n",
    "os.write(1, \"\\nSending to LCS: {}\".format(LCS_MESSAGING_ENABLED).encode())\n",
    "os.write(1, \"\\nIndividual interests destination name: {}\".format(INDIVIDUAL_LCS_DESTINATION_NAME).encode())\n",
    "os.write(1, \"\\nIndividual segment interests destination name: {}\".format(SEGMENT_LCS_DESTINATION_NAME).encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path Constants\n",
    "URL_IGNORE_LIST_PATH                         = './configuration/url-ignore-list.json'\n",
    "KEYWORD_IGNORE_LIST_PATH                     = './configuration/keyword-ignore-list.txt'\n",
    "USER_SEGMENT_LIST_PATH                       = './configuration/part-00000-2aa20d63-3e3a-47d2-8bed-d199cef5b814-c000.json'\n",
    "DATERANGE_CONFIGURATION_PATH                 = './configuration/daterange.txt'\n",
    "CUSTOMER_LIFERAY_MANUALLY_GENERATED_KEYWORDS = './configuration/customer-lr-manual-keywords.csv'\n",
    "WWW_LIFERAY_MANUALLY_GENERATED_KEYWORDS      = './configuration/www-lr-manual-keywords.csv'\n",
    "AMAZON_WEB_SERVICE_E3_BASE_FOLDER            = r'C:\\Users\\liferay\\Documents\\analytics data\\export'\n",
    "INDIVIDUAL_OUTPUT_DIRECTORY                  = './output/individual/'\n",
    "SEGMENT_OUTPUT_DIRECTORY                     = './output/segment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_TOPIC_OF_INTEREST_THRESHOLD_SCORE = .1\n",
    "DECAY_MULTIPLIER_BASE = .90\n",
    "\n",
    "# https://stackoverflow.com/questions/11066400/remove-punctuation-from-unicode-formatted-strings/11066687#11066687\n",
    "PUNCTUATION_UNICODE_TABLE = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "NON_ENGLISH_URL_REGEX = re.compile('\\/zh(_CN)?\\/'\n",
    "                                   '|\\/fr(_FR)?\\/'\n",
    "                                   '|\\/de(_DE)?\\/'\n",
    "                                   '|\\/it(_IT)?\\/'\n",
    "                                   '|\\/ja(_JP|-JP)?\\/'\n",
    "                                   '|\\/pt(-br|_BR|_PT)?\\/'\n",
    "                                   '|\\/es(-es|_ES)?\\/'\n",
    "                                   '|\\/ru\\/')\n",
    "WWW_OR_CUSTOMER_LIFERAY_URL_REGEX = re.compile(r'^https://www\\.liferay|^https://customer\\.liferay')\n",
    "BOT_AND_CRAWLER_REGEX = re.compile('((.*)(bot|Bot)(.*)'\n",
    "                                   '|(.*)spider(.*)'\n",
    "                                   '|(.*)crawler(.*)'\n",
    "                                   '|HubSpot'\n",
    "                                   '|CloudFlare\\-AlwaysOnline'\n",
    "                                   '|WkHTMLtoPDF)')\n",
    "PARENTHESIS_REGEX = re.compile(u'\\(.*?\\)')\n",
    "BANNED_KEYWORDS_LIST = []\n",
    "INTEREST_CALCULATION_WINDOW_TIMEDELTA = timedelta(30)\n",
    "\n",
    "DATE_RANGE_OPTIONS = {\n",
    "    'day'   : timedelta(1),\n",
    "    'week'  : timedelta(7),\n",
    "    'month' : timedelta(30)\n",
    "}\n",
    "\n",
    "UTM_PARAMETERS = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content']\n",
    "HUBSPOT_PARAMETERS = ['_hsenc', '_hsmi', '__hstc', '__hssc', '__hsfp']\n",
    "GOOGLE_ANALYTICS_PARAMETERS = ['_ga', '_gac']\n",
    "URL_REDIRECT_PARAMETERS = ['redirect', '_3_WAR_osbknowledgebaseportlet_redirect']\n",
    "ALL_OPTIONAL_URL_PARAMETERS = UTM_PARAMETERS + HUBSPOT_PARAMETERS + GOOGLE_ANALYTICS_PARAMETERS + URL_REDIRECT_PARAMETERS\n",
    "\n",
    "with open(KEYWORD_IGNORE_LIST_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        BANNED_KEYWORDS_LIST.append(line.strip())\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Populate URL Ignore List\n",
    "URL_IGNORE_LIST_MATCH = []\n",
    "URL_IGNORE_LIST_CONTAINS = []\n",
    "\n",
    "with open(URL_IGNORE_LIST_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_result = json.loads(line)      \n",
    "        comparison_type = json_result['Type']\n",
    "        \n",
    "        if comparison_type == 'match':\n",
    "            URL_IGNORE_LIST_MATCH = json_result['URLs']\n",
    "        elif comparison_type == 'contains':\n",
    "            URL_IGNORE_LIST_CONTAINS = json_result['URLs']\n",
    "        else:\n",
    "            print(\"UNEXPECTED TYPE: {}\".format(comparison_type))\n",
    "\n",
    "START_DATE_STRING = 0\n",
    "END_DATE_STRING = 0\n",
    "START_DATE_DATETIME = 0\n",
    "END_DATE_DATETIME = 0\n",
    "CALCULATE_YESTERDAY_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file for start/end dates\n",
    "with open(DATERANGE_CONFIGURATION_PATH, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "    # First parameter is to calculate 'all' or only 'yesterday' topics of interest\n",
    "    for line in f:\n",
    "        # Ignore lines starting with a pound-sign\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        else:\n",
    "            if line.strip() == 'yesterday':\n",
    "                CALCULATE_YESTERDAY_ONLY = True\n",
    "            break\n",
    "                \n",
    "    # Second parameter is the start date\n",
    "    for line in f:\n",
    "        # Ignore lines starting with a pound-sign\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        else:\n",
    "            START_DATE_STRING = line.strip()\n",
    "            START_DATE_DATETIME = datetime.strptime(line.strip(), '%Y%m%d')\n",
    "            break\n",
    "            \n",
    "    # Third parameter is for end date\n",
    "    for line in f:\n",
    "        # Ignore lines starting with a pound-sign\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        else:\n",
    "            if line == 'yesterday':\n",
    "                END_DATE_DATETIME = (datetime.today() - timedelta(1))\n",
    "                END_DATE_STRING = END_DATE_DATETIME.strftime('%Y%m%d')\n",
    "            else:\n",
    "                END_DATE_STRING = line.strip()\n",
    "                END_DATE_DATETIME = datetime.strptime(line.strip(), '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    print(START_DATE_STRING)\n",
    "    print(END_DATE_STRING)\n",
    "\n",
    "    print(START_DATE_DATETIME)\n",
    "    print(END_DATE_DATETIME)\n",
    "\n",
    "    print(CALCULATE_YESTERDAY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCS Messaging classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MessageBusMessage\n",
    "class MessageBusMessage(object):\n",
    "\n",
    "    def getDestinationName(self):\n",
    "        return self.destinationName\n",
    "\n",
    "    def getPayload(self):\n",
    "        return self.payload\n",
    "\n",
    "    def getMetadata(self):\n",
    "        return self.values\n",
    "\n",
    "    def setDestinationName(self, destination_name):\n",
    "        self.destinationName = destination_name\n",
    "\n",
    "    def setMetadata(self, metadata):\n",
    "        self.values = metadata\n",
    "\n",
    "    def setPayload(self, payload):\n",
    "        self.payload = payload\n",
    "\n",
    "    def toJSON(self):\n",
    "        obj_dict = self.__dict__\n",
    "        obj_dict[\"class\"] = \"com.liferay.lcs.messaging.MessageBusMessage\"\n",
    "\n",
    "        return json.dumps(obj_dict)\n",
    "\n",
    "    destinationName = None\n",
    "    values = None\n",
    "    payload = None\n",
    "\n",
    "# LCS Registry Client\n",
    "class LCSRegistryClient(object):\n",
    "\n",
    "    _LCS_REGISTRY_GET_INSTANCE_INFO_URL = \"/lcs-registry/get-instance-info/\"\n",
    "\n",
    "    def __init__(self, eureka_host, eureka_port, eureka_protocol='http'):\n",
    "        self._eureka_host = eureka_host\n",
    "        self._eureka_port = eureka_port\n",
    "        self._eureka_protocol = eureka_protocol\n",
    "\n",
    "    def getInstanceInfo(self, destination_name):\n",
    "        registry_url = self._getInstanceInfoURL(destination_name)\n",
    "\n",
    "        response = requests.get(registry_url)\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    def _getInstanceInfoURL(self, destination_name):\n",
    "        # double encode\n",
    "        encoded_destination = quote(destination_name, safe='')\n",
    "        encoded_destination = quote(encoded_destination, safe='')\n",
    "\n",
    "        return self._eureka_protocol + \"://\" + self._eureka_host + \":\" + str(self._eureka_port) + \\\n",
    "            self._LCS_REGISTRY_GET_INSTANCE_INFO_URL + \\\n",
    "            encoded_destination\n",
    "\n",
    "# LCS Message Bus\n",
    "class MessageBus(object):\n",
    "\n",
    "    _MESSAGE_BUS_SEND_MESSAGE_URL = \"/o/messagebus/send-message\"\n",
    "\n",
    "    def send(self, message_bus_message, destination_url):\n",
    "        messagebus_url = destination_url + self._MESSAGE_BUS_SEND_MESSAGE_URL\n",
    "\n",
    "        headers = {'Content-type': 'application/json'}\n",
    "\n",
    "        requests.post(messagebus_url, headers = headers, data=message_bus_message.toJSON())\n",
    "\n",
    "# LCS Message Bus Service\n",
    "class LCSMessageBusService(object):\n",
    "\n",
    "    _destination_cache = {}\n",
    "\n",
    "    def __init__(self, lcs_registry_client):\n",
    "        self._lcs_registry_client = lcs_registry_client\n",
    "        self._message_bus = MessageBus()\n",
    "\n",
    "    def sendMessage(self, destination_name, payload):\n",
    "        message_bus_message = MessageBusMessage()\n",
    "\n",
    "        message_bus_message.setDestinationName(destination_name)\n",
    "        message_bus_message.setPayload(payload)\n",
    "\n",
    "        destination_url = self._getDestinationURL(destination_name)\n",
    "\n",
    "        self._message_bus.send(message_bus_message, destination_url)\n",
    "\n",
    "    def _getDestinationURL(self, destination_name):\n",
    "        destination_url = self._destination_cache.get(destination_name)\n",
    "\n",
    "        if (destination_url == None):\n",
    "            instance_info = self._lcs_registry_client.getInstanceInfo(destination_name)\n",
    "\n",
    "            destination_url = instance_info.get(\"homePageUrl\")\n",
    "\n",
    "            self._destination_cache[destination_name] = destination_url\n",
    "\n",
    "        return destination_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (LCS_MESSAGING_ENABLED == True):\n",
    "    print(\"Instantiating LCS Message Bus service for registry: {}:{}\".format(EUREKA_SERVER_HOST, EUREKA_SERVER_PORT))\n",
    "    lcsRegistryClient = LCSRegistryClient(EUREKA_SERVER_HOST, EUREKA_SERVER_PORT)\n",
    "    lcsMessageBusService = LCSMessageBusService(lcsRegistryClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment Tokenizer\n",
    "The tokenizer fails on many hypenated words, so I wanted to augment it to work better.\n",
    "Examples: \n",
    "\n",
    "* State-of-the-art collaboration platform targets quality patient care.\n",
    "* Share files with a simple drag-and-drop. Liferay Sync transforms the Liferay platform into a central and secure easy-to-use document sharing service.\n",
    "* Importing/Exporting Pages and Content - portal - Knowledge | \"Liferay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.attrs import *\n",
    "\n",
    "#from spacy.symbols import ORTH, POS, TAG\n",
    "\n",
    "# Source: https://github.com/explosion/spaCy/issues/396\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp.tokenizer.add_special_case(u'state-of-the-art', [{ORTH: 'state-of-the-art',\n",
    "                                                      LEMMA: 'state-of-the-art', \n",
    "                                                      LOWER: 'state-of-the-art',\n",
    "                                                      SHAPE: 'xxxxxxxxxxxxxxxx',\n",
    "                                                      POS: 'ADJ', \n",
    "                                                      TAG: 'JJ'}])\n",
    "nlp.tokenizer.add_special_case(u'State-of-the-art', [{ORTH: 'State-of-the-art',\n",
    "                                                      LEMMA: 'state-of-the-art', \n",
    "                                                      LOWER: 'state-of-the-art',\n",
    "                                                      SHAPE: 'xxxxxxxxxxxxxxxx',\n",
    "                                                      POS: 'ADJ', \n",
    "                                                      TAG: 'JJ'}])\n",
    "nlp.tokenizer.add_special_case(u'drag-and-drop', [{ORTH: 'drag-and-drop',\n",
    "                                                      LEMMA: 'drag-and-drop', \n",
    "                                                      LOWER: 'drag-and-drop',\n",
    "                                                      SHAPE: 'xxxxxxxxxxxxx',\n",
    "                                                      POS: 'ADJ', \n",
    "                                                      TAG: 'JJ'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Functions\n",
    "\n",
    "import re\n",
    "import langdetect\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "from langdetect.lang_detect_exception import ErrorCode, LangDetectException\n",
    "from string import printable\n",
    "\n",
    "\n",
    "def replace_punctuation(text):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to replace non-ASCII punctuation with its equivalent.\n",
    "    \"\"\"\n",
    "    return text.replace(\"?\", \"'\")\n",
    "\n",
    "def segmentWordsIntoKeyWordPhraseList(words, debug=False):\n",
    "\n",
    "    phrase_list = []\n",
    "    \n",
    "    if debug: print(\"\\nOriginal Sentence: {}\".format(words))\n",
    "    # First segment the words by '|' or '-'\n",
    "    split_words = re.split(r'[\\|]| \\- ', words)\n",
    "    split_words = [s.strip() for s in split_words]\n",
    "    cleaned_up_and_split_sentences = []\n",
    "    \n",
    "    # Search for instances of acronymns surrounded in parenthesis. Ex: (DXP)\n",
    "    # Remove those, and add it automatically to the phrase list\n",
    "    for sentence in split_words:\n",
    "        terms_within_parenthesis = [term[1:-1] for term in re.findall(PARENTHESIS_REGEX, sentence)]\n",
    "        phrase_list += terms_within_parenthesis\n",
    "        if debug: print(terms_within_parenthesis)\n",
    "            \n",
    "        remaining_text = ''.join(re.split(PARENTHESIS_REGEX, sentence))\n",
    "        cleaned_up_and_split_sentences.append(remaining_text)\n",
    "        if debug: print(remaining_text)\n",
    "        \n",
    "    for sentence in cleaned_up_and_split_sentences:\n",
    "        if debug: print(\"Sentence: {}\".format(sentence))\n",
    "        doc = nlp(sentence)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if debug: print(\"\\tText: {} \\n\\tRoot: {} \\n\\tRoot Dependency: {} \\n\\tRoot Head: {}\".format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))\n",
    "            \n",
    "            text = chunk.text\n",
    "            if debug:\n",
    "                print(text)\n",
    "                print(\"\\tPOS: {}\".format(chunk[0].pos_))\n",
    "                print(\"\\tTag: {}\".format(chunk[0].tag_))\n",
    "                print(\"\\tChunk[0]: {}\".format(chunk[0]))\n",
    "                \n",
    "            # Skip keywords that contain CD (Cardinal Numbers) for now\n",
    "            if 'CD' in [c.tag_ for c in chunk]:\n",
    "                print(\"Skipping, contains CD\")\n",
    "                continue\n",
    "            \n",
    "            # Skip URLs\n",
    "            url_found = False\n",
    "            for token in chunk:\n",
    "                if debug: print(token)\n",
    "                if token.like_url:\n",
    "                    url_found = True\n",
    "                    print(\"Skipping, URL Detected! ({})\".format(text))\n",
    "                    break\n",
    "                    \n",
    "            if url_found:\n",
    "                continue\n",
    "            \n",
    "            # We'll skip a phrase for now if it contains a number\n",
    "            # E.g. Free download: Gartner's evaluation of 21 digital \n",
    "            # experience platform (DXP) providers based on their completeness of vision and ability to execute\n",
    "            \n",
    "            # CD - [5 Critical Things] Nobody Tells You About Building a Journey Map\n",
    "            # Recursively remove until no more? - These six customer experience trends will shape business in 2018\n",
    "            if chunk[0].tag_ in ['DT', 'PRP$', 'WP', 'PRP', 'WRB', 'CD', ':']:\n",
    "                if debug: print(\"Starting 'ignore word' found in: {}\".format(chunk))\n",
    "                #text = ' '.join(s.text for s in chunk[1:])\n",
    "                \n",
    "                unwanted_text = chunk[0].text\n",
    "                if debug: print(\"Unwanted text: {}\".format(unwanted_text))\n",
    "                text = chunk[1:].text\n",
    "                \n",
    "                # If we shrunk it down to nothing\n",
    "                if not text:\n",
    "                    continue\n",
    "            \n",
    "            # Removes invisible characters\n",
    "            printable_string = ''.join(char for char in text.strip() if char in printable)\n",
    "            \n",
    "            # Converts string to lower case; if matches criteria\n",
    "            # Note: Keep acroynmns the same, check if 2 or more letters, and all caps\n",
    "            printable_string = modifyCapitalizationOfWords(printable_string)\n",
    "            \n",
    "            #if 'blog' in printable_string:\n",
    "            #    print(\"Original Sentence: [{}]\".format(words))\n",
    "            #    print(\"Blog Word: [{}]\".format(printable_string))\n",
    "            \n",
    "            if text == chunk.root.text:\n",
    "                phrase_list.append(printable_string)\n",
    "            else:\n",
    "                phrase_list.append(printable_string)\n",
    "                #phrase_list.append(chunk.root.text.lower())\n",
    "            \n",
    "    if debug: print(\"Final list: {}\".format(phrase_list))\n",
    "    return phrase_list\n",
    "    \n",
    "def modifyCapitalizationOfWords(text):\n",
    "    \"\"\"\n",
    "    This function will take the given noun phrase, and adjust captialization as necessary.\n",
    "    Currently it only retains acronymn capitalization.\n",
    "    I should ventually add a proper noun list as well.\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_text = [word if (len(word) >=2) and (word.upper() == word) else word.lower() for word in text.split()]\n",
    "    \n",
    "    return ' '.join(updated_text)\n",
    "    \n",
    "def isEnglish(text, debug=False):\n",
    "    \n",
    "    # Empty String\n",
    "    if not text.strip():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        text.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        if debug:\n",
    "            print(\"Failed Unicode Detector\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        possible_language_list = langdetect.detect_langs(text)\n",
    "        \n",
    "        if debug:\n",
    "            print(possible_language_list)\n",
    "        \n",
    "        for entry in possible_language_list:\n",
    "            if ((entry.lang == 'en') and (entry.prob > .50)):\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    except LangDetectException:\n",
    "        print(\"**** Language Exception caught!\")\n",
    "        display(\"Original Text: [{}]\".format(text))\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_date_folders(start_date='20180227', end_date='20180326'):\n",
    "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "    step = timedelta(days=1)\n",
    "\n",
    "    list_of_date_folder_names = []\n",
    "\n",
    "    while start_date <= end_date:\n",
    "        date_string = start_date.date().strftime('%Y%m%d')\n",
    "        list_of_date_folder_names.append(date_string)\n",
    "        start_date += step\n",
    "\n",
    "    return list_of_date_folder_names\n",
    "\n",
    "def read_json_as_list(full_file_path):\n",
    "    all_web_browsing_history = []\n",
    "\n",
    "    with open(full_file_path, 'r', encoding='utf-8') as f:\n",
    "        for counter, line in enumerate(f):\n",
    "            dict_entry = json.loads(line)       \n",
    "            all_web_browsing_history.append(dict_entry)\n",
    "                \n",
    "    return all_web_browsing_history\n",
    "                \n",
    "\n",
    "def convert_string_of_json_to_df(list_of_json):\n",
    "    start_time = datetime.now()\n",
    "    df = json_normalize(list_of_json)\n",
    "    print(\"\\tExecution Time: {}\".format(datetime.now() - start_time))\n",
    "    return df\n",
    "\n",
    "def get_s3_keys(s3, bucket):\n",
    "    \"\"\"Get a list of keys in an S3 bucket.\"\"\"\n",
    "    keys = []\n",
    "    resp = s3.list_objects_v2(Bucket=bucket)\n",
    "    for obj in resp['Contents']:\n",
    "        keys.append(obj['Key'])\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Segment Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate Segment Information\n",
    "segment_lookup_df = pd.DataFrame()\n",
    "json_list = read_json_as_list(USER_SEGMENT_LIST_PATH)\n",
    "segment_lookup_df = json_normalize(json_list)\n",
    "display(segment_lookup_df)\n",
    "segment_lookup_df = segment_lookup_df.set_index(['identifier', 'datasource', 'datasourceindividualpk'])['segmentnames'].apply(pd.Series).stack()\n",
    "segment_lookup_df = pd.DataFrame(segment_lookup_df)\n",
    "segment_lookup_df = segment_lookup_df.reset_index().rename(columns={0 : 'segmentName'})\n",
    "\n",
    "# Switch order of columns\n",
    "segment_lookup_df = segment_lookup_df[['segmentName', 'identifier', 'datasource', 'datasourceindividualpk']]\n",
    "\n",
    "\n",
    "if False:\n",
    "    display(temp_df)\n",
    "    for index, row in temp_df.groupby('segmentName'):\n",
    "        print(\"index\")\n",
    "        display(index)\n",
    "        print(\"row\")\n",
    "        display(row)\n",
    "        print(\"identifier\")\n",
    "        display(row['identifier'])\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: display(segment_lookup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from furl import furl\n",
    "\n",
    "def show_dataframe_length_before_and_after(f, df):\n",
    "    print(\"\\tBefore: {}\".format(len(df)))\n",
    "    df = f(df)\n",
    "    print(\"\\tAfter: {}\".format(len(df)))\n",
    "    return df\n",
    "\n",
    "def keep_only_unload_events(df):\n",
    "    df = df[df['eventid'] == 'unload']\n",
    "    return df\n",
    "\n",
    "def remove_all_bots(df):\n",
    "    df = df[~df['context.crawler'].str.contains('True', na=False)]\n",
    "    df = df[~df['context.userAgent'].str.match(BOT_AND_CRAWLER_REGEX, na=False)]\n",
    "    return df\n",
    "\n",
    "def remove_non_english_urls(df):\n",
    "    df = df[~df['context.url'].str.contains(NON_ENGLISH_URL_REGEX, na=False)]\n",
    "    return df\n",
    "\n",
    "def populate_url_ignore_list(df):\n",
    "    \n",
    "    URL_IGNORE_LIST_MATCH_REGEX_STRING    = '|'.join(['^{}$'.format(s.strip()) for s in URL_IGNORE_LIST_MATCH])\n",
    "    URL_IGNORE_LIST_CONTAINS_REGEX_STRING = '|'.join(URL_IGNORE_LIST_CONTAINS)\n",
    "\n",
    "    # TODO: Maybe use 'normalized_url' only?\n",
    "    df['Ignore URL'] = df['context.url'].str.match(URL_IGNORE_LIST_MATCH_REGEX_STRING) \\\n",
    "                     | df['context.og:url'].str.match(URL_IGNORE_LIST_MATCH_REGEX_STRING) \\\n",
    "                     | df['context.url'].str.match(URL_IGNORE_LIST_CONTAINS_REGEX_STRING) \\\n",
    "                     | df['context.og:url'].str.match(URL_IGNORE_LIST_CONTAINS_REGEX_STRING)\n",
    "    return df\n",
    "\n",
    "def remove_non_customer_www_lr_urls(df):\n",
    "    df = df[df['context.url'].str.contains(WWW_OR_CUSTOMER_LIFERAY_URL_REGEX, na=False)]\n",
    "    return df\n",
    "\n",
    "def remove_empty_user_id_entries(df):\n",
    "    df['userid'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['userid'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def __removeUrlParameters(url, parameter_list):  \n",
    "    f = furl(url)\n",
    "    remaining_parameters = { k: f.args[k] for k in f.args if k not in parameter_list }\n",
    "    f.args = remaining_parameters    \n",
    "    return f.url\n",
    "\n",
    "def populateNormalizedUrlField(df):\n",
    "    df['normalized_url'] = df['context.og:url'].fillna(df['context.url'])\n",
    "    df['normalized_url'] = df['normalized_url'].apply(lambda x: __removeUrlParameters(x, ALL_OPTIONAL_URL_PARAMETERS))\n",
    "    return df\n",
    "\n",
    "def replaceBlankSpacesWithNan(df):\n",
    "    # '\\s+' is 1 or more\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def filterUnwantedColumns(df):\n",
    "    wanted_columns_list = ['eventdate', \n",
    "                           'analyticskey', \n",
    "                           'userid', \n",
    "                           'eventid', \n",
    "                           'Ignore URL',\n",
    "                           'normalized_url',\n",
    "                           'context.url', 'context.og:url', \n",
    "                           'context.title', 'context.og:title', \n",
    "                           'context.description', 'context.og:description', \n",
    "                           'context.keywords',\n",
    "                           'context.contentLanguageId',\n",
    "                           'eventproperties.scrollDepth', \n",
    "                           'eventproperties.viewDuration', \n",
    "                           'context.userAgent', \n",
    "                           'context.platformName', \n",
    "                           'context.browserName', \n",
    "                           'context.country', \n",
    "                           'context.region', \n",
    "                           'context.city', \n",
    "                           'clientip']\n",
    "    df = df[wanted_columns_list]\n",
    "    return df\n",
    "\n",
    "def convertColumnsToAppropriateDataTypes(df):\n",
    "    print(\"Converting eventdate to datetime objects\")\n",
    "    df['eventdate'] = pd.to_datetime(df['eventdate'])\n",
    "    print(\"Converting viewDuration to int\")\n",
    "    df['eventproperties.viewDuration'] = pd.to_numeric(df['eventproperties.viewDuration'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files and save as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This is in case an exception is thrown when accessing AWS, E3 data\n",
    "RETRY_LIMIT = 3\n",
    "\n",
    "# Plan go through list of directories, and parse in all the relevant JSON files.\n",
    "BASE_S3_FILE_DIRECTORY = 's3://lcs-ec-analytics/'\n",
    "start_date = START_DATE_STRING\n",
    "end_date = END_DATE_STRING\n",
    "\n",
    "list_of_date_folder_names = get_list_of_date_folders(start_date=start_date, end_date=end_date)\n",
    "s3 = boto3.client('s3')\n",
    "keys_list = get_s3_keys(s3, 'lcs-ec-analytics')\n",
    "full_df = pd.DataFrame()\n",
    "\n",
    "if True:\n",
    "    os.write(1, \"\\nReading JSON files from AWS S3\".encode())\n",
    "    # For reading from AWS S3 sever\n",
    "    for sub_folder_name in list_of_date_folder_names:\n",
    "        matching_s3_file_list = [match for match in keys_list if ((sub_folder_name in match) \n",
    "                                                                  and (match.endswith('.json')) \n",
    "                                                                  and ('user_individual_segment' not in match))]\n",
    "\n",
    "\n",
    "        if len(matching_s3_file_list) == 0:\n",
    "            print(\"[WARNING] - No entries found for this date: {}\".format(sub_folder_name))\n",
    "            print(\"\\n{}\".format(matching_s3_file_list))\n",
    "            os.write(1, \"[WARNING] - No entries found for this date: {}\".format(sub_folder_name).encode())\n",
    "            os.write(1, \"\\n{}\".format(matching_s3_file_list).encode())\n",
    "            continue\n",
    "        elif len(matching_s3_file_list) >= 2:\n",
    "            print(\"[WARNING] - More than 1 entry found for this date: {}\".format(sub_folder_name))\n",
    "            print(\"Only reading from 1st entry\")\n",
    "            print(\"\\n{}\".format(matching_s3_file_list))\n",
    "            os.write(1, \"[WARNING] - More than 1 entry found for this date: {}\".format(sub_folder_name).encode())\n",
    "            continue\n",
    "\n",
    "        file_path = BASE_S3_FILE_DIRECTORY + matching_s3_file_list[0]\n",
    "        print(\"\\n{}\".format(file_path))\n",
    "        os.write(1, \"\\n\\t\\n{}\".format(file_path).encode())\n",
    "        \n",
    "        attempts = 0\n",
    "\n",
    "        while attempts < RETRY_LIMIT:\n",
    "            try:\n",
    "                json_list = []\n",
    "                for line in smart_open.smart_open(file_path):\n",
    "                    line = line.decode(\"utf-8\").strip()\n",
    "                    dict_entry = json.loads(line)       \n",
    "                    json_list.append(dict_entry)\n",
    "\n",
    "                print(\"\\tEntries: {}\".format(len(json_list)))\n",
    "\n",
    "                if len(json_list) == 0:\n",
    "                    print(\"\\t[WARNING] 0 entries detected! Skipping...\")\n",
    "                    os.write(1, \"\\t[WARNING] 0 entries detected! Skipping...\".encode())\n",
    "                    continue\n",
    "\n",
    "                df = convert_string_of_json_to_df(json_list)\n",
    "\n",
    "                # XXX: Workaround to improve memory usage\n",
    "                df = keep_only_unload_events(df)\n",
    "\n",
    "                full_df = full_df.append(df, ignore_index=True)\n",
    "                break\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"KeyboardInterrupt detected. Aborting...\")\n",
    "                raise KeyboardInterrupt\n",
    "            except Exception as e: \n",
    "                print(str(e))\n",
    "                print(\"Unexpected error detected!\")                \n",
    "                attempts += 1\n",
    "                \n",
    "                if attempts < RETRY_LIMIT:\n",
    "                    print(\"\\tRetrying ({})...\".format(attempts))\n",
    "                    os.write(1, \"\\tRetrying ({})...\".format(attempts).encode())\n",
    "                else:\n",
    "                    print(\"\\t[ERROR] - Unable to read file after {} retries! Skipping...\".format(RETRY_LIMIT))\n",
    "                    os.write(1, \"\\t[ERROR] - Unable to read file after {} retries! Skipping...\".format(RETRY_LIMIT).encode())\n",
    "    \n",
    "# Only execute this if we're reading the JSON files locally\n",
    "else:\n",
    "    for sub_folder_name in list_of_date_folder_names:\n",
    "        directory_name = os.path.join(AMAZON_WEB_SERVICE_E3_BASE_FOLDER, sub_folder_name)\n",
    "\n",
    "        for filename in os.listdir(directory_name):\n",
    "            full_directory_and_file_name = os.path.join(directory_name, filename)\n",
    "\n",
    "            if filename.endswith(\".json\"): \n",
    "                try:\n",
    "                    print(\"\\n{}\".format(full_directory_and_file_name))\n",
    "                    json_list = read_json_as_list(full_directory_and_file_name)\n",
    "                    print(\"\\tEntries: {}\".format(len(json_list)))\n",
    "                    \n",
    "                    if len(json_list) == 0:\n",
    "                        print(\"[WARNING] 0 entries detected! Skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    \n",
    "                    df = convert_string_of_json_to_df(json_list)\n",
    "\n",
    "                    # XXX: Workaround to improve memory usage\n",
    "                    df = keep_only_unload_events(df)\n",
    "\n",
    "                    full_df = full_df.append(df, ignore_index=True)\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"KeyboardInterrupt detected. Aborting...\")\n",
    "                except:\n",
    "                    print(\"Unexpected error detected!\") \n",
    "                    \n",
    "os.write(1, \"\\nFinished\".encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "    display(full_df['context.contentLanguageId'].unique())\n",
    "    display(full_df[full_df['context.contentLanguageId'] == 'ru-RU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "    display(full_df['context.contentLanguageId'].unique())\n",
    "    display(full_df[full_df['context.contentLanguageId'] == 'ru-RU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "\n",
    "os.write(1, \"\\nStarting ETL Process\\n\".encode())\n",
    "\n",
    "# Surpress Warning Messages from \"removing non-English URLs\"\n",
    "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n",
    "\n",
    "print(\"Keeping only UNLOAD events\")\n",
    "etl_df = show_dataframe_length_before_and_after(keep_only_unload_events, full_df)\n",
    "\n",
    "print(\"Removing Bots\")\n",
    "etl_df = show_dataframe_length_before_and_after(remove_all_bots, etl_df)\n",
    "\n",
    "print(\"Removing Non-English URLs\")\n",
    "etl_df = show_dataframe_length_before_and_after(remove_non_english_urls, etl_df)\n",
    "\n",
    "print(\"Removing non-customer, non-www URLs\")\n",
    "etl_df = show_dataframe_length_before_and_after(remove_non_customer_www_lr_urls, etl_df)\n",
    "\n",
    "print(\"Removing empty userid entries\")\n",
    "etl_df = show_dataframe_length_before_and_after(remove_empty_user_id_entries, etl_df)\n",
    "\n",
    "print(\"Populating normalized_url field\")\n",
    "etl_df = populateNormalizedUrlField(etl_df)\n",
    "\n",
    "print(\"Populating URL Ignore List\")\n",
    "etl_df = show_dataframe_length_before_and_after(populate_url_ignore_list, etl_df)\n",
    "print(\"Ignoring {} URLs\".format(len(etl_df[etl_df['Ignore URL'] == True])))\n",
    "\n",
    "print(\"Removing unwanted columns\")\n",
    "etl_df = filterUnwantedColumns(etl_df)\n",
    "\n",
    "print(\"Converting columns to appropriate data types\")\n",
    "etl_df = convertColumnsToAppropriateDataTypes(etl_df)\n",
    "\n",
    "print(\"Replacing blank spaces with NaN\")\n",
    "etl_df = replaceBlankSpacesWithNan(etl_df)\n",
    "\n",
    "os.write(1, \"\\n\\tFinished ETL\\n\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Make a copy, and use it\n",
    "clean_df = etl_df.copy()\n",
    "clean_df.sort_values('eventdate')\n",
    "display(\"Length: {}\".format(len(clean_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save URLs for Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable for production (for now)\n",
    "if False:\n",
    "    url_s = pd.Series(clean_df['normalized_url'].unique()).sort_values()\n",
    "    print(\"Number of URLs: {}\".format(len(url_s)))\n",
    "    url_s.to_csv('./output/Unique Visitor URLs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame: URL Lookup Information\n",
    "This will be the centralized URL to information Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.write(1, \"\\tPopulating Lookup Tables\".encode())\n",
    "url_to_title          = clean_df.groupby(['normalized_url'])['context.title'].apply(set)\n",
    "url_to_og_title       = clean_df.groupby(['normalized_url'])['context.og:title'].apply(set)\n",
    "url_to_description    = clean_df.groupby(['normalized_url'])['context.description'].apply(set)\n",
    "url_to_og_description = clean_df.groupby(['normalized_url'])['context.og:description'].apply(set)\n",
    "url_to_keywords       = clean_df.groupby(['normalized_url'])['context.keywords'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUrlToKeywordDf():\n",
    "    columns = ['normalized_url',\n",
    "           'analyticsclient.merged_title', \n",
    "           'analyticsclient.merged_description', \n",
    "           'analyticsclient.merged_keywords',\n",
    "           'analyticsclient.generated_keywords']\n",
    "\n",
    "    url_to_keyword_df = pd.DataFrame(columns=columns)\n",
    "    url_to_keyword_df['normalized_url'] = clean_df['normalized_url'].unique()\n",
    "    #display(url_to_keyword_df)\n",
    "    return url_to_keyword_df\n",
    "\n",
    "def generateKeywordsFromTitleDescriptionKeywords(title, og_title, description, og_description, keywords, debug=False):\n",
    "    merged_title = title.union(og_title)\n",
    "    merged_description = description.union(og_description)\n",
    "    \n",
    "    keywords_from_title = set()\n",
    "    keywords_from_description = set()\n",
    "    keywords_from_keywords = set()\n",
    "    \n",
    "    only_english_titles = set()\n",
    "    only_english_descriptions = set()\n",
    "    only_english_keyword_set = set()\n",
    "    \n",
    "    title_description_to_keyword_cache = defaultdict(int)\n",
    "\n",
    "    for entry in merged_title:\n",
    "\n",
    "        # Skip empty strings       \n",
    "        if pd.isnull(entry):\n",
    "            continue\n",
    "            \n",
    "        # remove weird HTML punct\n",
    "        entry = replace_punctuation(entry)\n",
    "        \n",
    "        cached_result = title_description_to_keyword_cache[entry]\n",
    "        \n",
    "        if cached_result != 0:\n",
    "            keywords_from_title.update(cached_result)\n",
    "            only_english_titles.update([entry])\n",
    "        elif isEnglish(entry):\n",
    "            #print(\"isEnglish() passed\")\n",
    "            #print(\"entry: \", entry)\n",
    "            keyword_phrase_list = segmentWordsIntoKeyWordPhraseList(entry, debug=False)\n",
    "            keywords_from_title.update(keyword_phrase_list)\n",
    "            only_english_titles.update([entry])\n",
    "            #print(\"entry: {}\".format(entry))\n",
    "            #print(\"only_english_titles: {}\".format(only_english_titles))\n",
    "            \n",
    "            # Update Cache:\n",
    "            title_description_to_keyword_cache[entry] = keyword_phrase_list\n",
    "        else:\n",
    "            print(\"Non-English detected: [{}]\".format(entry))\n",
    "            title_description_to_keyword_cache[entry] = []\n",
    "            continue\n",
    "    \n",
    "    for entry in merged_description:        \n",
    "        # Skip empty strings\n",
    "        if pd.isnull(entry):\n",
    "            continue\n",
    "            \n",
    "        # remove punct\n",
    "        entry = replace_punctuation(entry)\n",
    "        \n",
    "        cached_result = title_description_to_keyword_cache[entry]\n",
    "        \n",
    "        if cached_result != 0:\n",
    "            keywords_from_description.update(cached_result)\n",
    "            only_english_descriptions.update([entry])\n",
    "        elif isEnglish(entry):\n",
    "            keyword_phrase_list = segmentWordsIntoKeyWordPhraseList(entry)\n",
    "            keywords_from_description.update(keyword_phrase_list)\n",
    "            only_english_descriptions.update([entry])\n",
    "            \n",
    "            # Update Cache:\n",
    "            title_description_to_keyword_cache[entry] = keyword_phrase_list\n",
    "        else:\n",
    "            print(\"Non-English detected: [{}]\".format(entry))\n",
    "            title_description_to_keyword_cache[entry] = []\n",
    "            continue\n",
    "        \n",
    "    for entry in keywords:\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if pd.isnull(entry):\n",
    "            continue\n",
    "            \n",
    "        if isEnglish(entry):\n",
    "            split_list = [s.strip() for s in entry.split(',')]\n",
    "            keywords_from_keywords.update(set(split_list if split_list else []))\n",
    "            only_english_keyword_set.update(set(split_list if split_list else []))\n",
    "        else:\n",
    "            print(\"Non-English detected: [{}]\".format(entry))\n",
    "            continue\n",
    "    \n",
    "    # Debugging\n",
    "    if debug:\n",
    "        print(\"\\n\\tMerged Title: {} => {}\".format(only_english_titles, keywords_from_title))\n",
    "        print(\"\\tMerged Descr: {} => {}\".format(only_english_descriptions, keywords_from_description))\n",
    "        print(\"\\tKeywords:     {} => {}\".format(only_english_keyword_set, keywords_from_keywords))\n",
    "        \n",
    "    # merge all sets together\n",
    "    all_keywords_merged = keywords_from_keywords.union(keywords_from_title, keywords_from_description)\n",
    "    if debug: print(\"\\tAll Keywords: {}\".format(all_keywords_merged))\n",
    "\n",
    "    # We return the English list of inputs we processed, and the final keyword output\n",
    "    return list(only_english_titles), list(only_english_descriptions), list(only_english_keyword_set), list(all_keywords_merged)\n",
    "\n",
    "def populateUrlToKeywordDf(url_to_keyword_df, debug=False):\n",
    "    unique_url_list = url_to_keyword_df['normalized_url'].unique()\n",
    "\n",
    "    for counter, url in enumerate(unique_url_list):       \n",
    "        title = url_to_title.get(url, set())\n",
    "        og_title = url_to_og_title.get(url, set())\n",
    "        description = url_to_description.get(url, set())\n",
    "        og_description = url_to_og_description.get(url, set())\n",
    "        keywords_set = url_to_keywords.get(url, set())\n",
    "\n",
    "        if debug: \n",
    "            print('\\n{} / {}'.format(counter, len(unique_url_list)))\n",
    "            print('{}'.format(url))\n",
    "        merged_title_list, merged_description_list, merged_keyword_list, generated_keyword_list = generateKeywordsFromTitleDescriptionKeywords(title, og_title, description, og_description, keywords_set)\n",
    "\n",
    "        # Populate url_to_keyword_df, with keywords\n",
    "        index = url_to_keyword_df.loc[url_to_keyword_df['normalized_url'] == url]\n",
    "        if len(index.index.values) > 1:\n",
    "            print(\"ERROR: There shouldn't be more than 1 entry for the URL list!\")\n",
    "            print(\"index: {}\".format(index))\n",
    "            print(\"index.index.values: {}\".format(index.index.values))\n",
    "            break\n",
    "\n",
    "        if len(merged_title_list) > 0: \n",
    "            url_to_keyword_df.at[index.index.values[0], 'analyticsclient.merged_title'] = merged_title_list\n",
    "\n",
    "        if len(merged_description_list) > 0: \n",
    "            url_to_keyword_df.at[index.index.values[0], 'analyticsclient.merged_description'] = merged_description_list\n",
    "\n",
    "        if len(merged_keyword_list) > 0: \n",
    "            url_to_keyword_df.at[index.index.values[0], 'analyticsclient.merged_keywords'] = merged_keyword_list\n",
    "\n",
    "        url_to_keyword_df.at[index.index.values[0], 'analyticsclient.generated_keywords'] = generated_keyword_list\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            print(\"{} / {}\".format(counter, len(unique_url_list)))\n",
    "        \n",
    "    return url_to_keyword_df\n",
    "\n",
    "def addKeywordBoosting(df, debug=True):\n",
    "    www_lr_manual_keywords = pd.read_csv(WWW_LIFERAY_MANUALLY_GENERATED_KEYWORDS)\n",
    "    customer_lr_manual_keywords = pd.read_csv(CUSTOMER_LIFERAY_MANUALLY_GENERATED_KEYWORDS)\n",
    "    all_lr_manual_keywords = www_lr_manual_keywords.append(customer_lr_manual_keywords, ignore_index=True)\n",
    "    all_lr_manual_keywords = all_lr_manual_keywords[['URL', 'Keywords']]\n",
    "    all_lr_manual_keywords = all_lr_manual_keywords.dropna(how='any')\n",
    "    all_lr_manual_keywords['Keywords'] = all_lr_manual_keywords['Keywords'].apply(lambda x: \n",
    "                                               [modifyCapitalizationOfWords(s.strip()) for s in x.split(',') if s.strip()])\n",
    "\n",
    "    # Populate existing url-to-keyword lookup dataframe\n",
    "    temp_df = pd.merge(df, all_lr_manual_keywords, how='left', left_on='normalized_url', right_on='URL')\n",
    "\n",
    "    # Rename the \"Keywords\" column to \"manual.keywords\"\n",
    "    temp_df.rename(columns={'Keywords' : 'manual.keywords'}, inplace=True)\n",
    "\n",
    "    # Rearrange order of columns\n",
    "    temp_df = temp_df[['normalized_url',\n",
    "                       'analyticsclient.generated_keywords',\n",
    "                       'manual.keywords',\n",
    "                       'analyticsclient.merged_title',\n",
    "                       'analyticsclient.merged_description', \n",
    "                       'analyticsclient.merged_keywords']]\n",
    "\n",
    "    # Replace analyticsclient.generated_keywords [] with NaN\n",
    "    temp_df.loc[temp_df['analyticsclient.generated_keywords'].str.len() == 0, 'analyticsclient.generated_keywords'] = np.nan\n",
    "\n",
    "    # Filter out URLs where the \"Automatically Generated Keywords\" or \"Manually generated Keywords\" are missing\n",
    "    if debug:\n",
    "        print(\"Removing entries where both auto & manually generated keywords are missing\")\n",
    "        print(\"Before: {}\".format(len(temp_df)))\n",
    "    \n",
    "    with pd.option_context('display.max_rows', 200, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "        display(temp_df)\n",
    "\n",
    "    temp_df = temp_df[(~temp_df['analyticsclient.generated_keywords'].isnull()) | (~temp_df['manual.keywords'].isnull())]\n",
    "\n",
    "    #with pd.option_context('display.max_rows', 200, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "    #    display(temp_df)\n",
    "\n",
    "    if debug:\n",
    "        print(\"After: {}\".format(len(temp_df)))\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "def generateUrlToKeywordDict(df, keyword_types=[''], use_banned_word_list=True, debug=True):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    There will be multiple options for what type of keywords you can select from\n",
    "    * manual - these are the tags manually added (there aren't that many of these)\n",
    "    * title_description_keyword - these are the tags provided by the metadata\n",
    "    * web_scraping - these are the tags generated by web scraping\n",
    "    \"\"\"\n",
    "    import numpy\n",
    "    \n",
    "    # Add new empty column to df, for storing the combined keywords\n",
    "    df['combined keywords'] = np.nan\n",
    "    df['combined keywords'] = df['combined keywords'].astype(object)\n",
    "    \n",
    "    url_s = df['normalized_url'].unique()\n",
    "    url_lookup_cache = dict()\n",
    "    no_keywords_urls = []\n",
    "    \n",
    "    for counter, url in enumerate(url_s):\n",
    "        \n",
    "        if debug: \n",
    "            print(\"\\n{} / {} - {}\".format(counter, len(url_s), url))\n",
    "        generated_keyword_list          = df.loc[df['normalized_url'] == url]['analyticsclient.generated_keywords'].values.tolist()\n",
    "        manually_populated_keyword_list = df.loc[df['normalized_url'] == url]['manual.keywords'].values.tolist()\n",
    "\n",
    "        # Filter [nan] scenarios\n",
    "        if numpy.nan in generated_keyword_list:\n",
    "            generated_keyword_list = []\n",
    "        elif len(generated_keyword_list) >= 1:\n",
    "            generated_keyword_list = generated_keyword_list[0]\n",
    "\n",
    "        if numpy.nan in manually_populated_keyword_list:\n",
    "            manually_populated_keyword_list = []\n",
    "        elif len(manually_populated_keyword_list) >= 1:\n",
    "            manually_populated_keyword_list = manually_populated_keyword_list[0]\n",
    "\n",
    "        aggregate_keyword_list = list(set(generated_keyword_list).union(set(manually_populated_keyword_list)))\n",
    "\n",
    "        if use_banned_word_list:\n",
    "            aggregate_keyword_list = [w for w in aggregate_keyword_list if w.lower() not in BANNED_KEYWORDS_LIST]\n",
    "\n",
    "        # Cache result\n",
    "        url_lookup_cache[url] = aggregate_keyword_list\n",
    "        if debug:\n",
    "            print(\"\\t{}\".format(aggregate_keyword_list))\n",
    "        \n",
    "        if not aggregate_keyword_list:\n",
    "            print(\"\\tWarning: [{}] has 0 entries!\".format(url))\n",
    "            no_keywords_urls.append(url)\n",
    "            \n",
    "        # Add the entry back to the dataframe     \n",
    "        index = df.loc[df['normalized_url'] == url]\n",
    "        df.at[index.index.values[0], 'combined keywords'] = aggregate_keyword_list\n",
    "            \n",
    "    return url_lookup_cache, df, no_keywords_urls\n",
    "\n",
    "# For Debugging\n",
    "def lookUpKeywordBreakdownBasedOnUrl(url):\n",
    "    title = url_to_title.get(url, set())\n",
    "    og_title = url_to_og_title.get(url, set())\n",
    "    description = url_to_description.get(url, set())\n",
    "    og_description = url_to_og_description.get(url, set())\n",
    "    keywords_set = url_to_keywords.get(url, set())\n",
    "    \n",
    "    print(\"Title: {}\".format(title))\n",
    "    print(\"og_title: {}\".format(og_title))\n",
    "    print(\"description: {}\".format(description))\n",
    "    print(\"og_description: {}\".format(og_description))\n",
    "    print(\"keywords: {}\".format(keywords_set))\n",
    "    \n",
    "def generateKeywordToIndividualKeywordList(url_to_keyword_df):\n",
    "    url_to_keyword_df = url_to_keyword_df[['normalized_url', 'combined keywords']]\n",
    "\n",
    "    # Expand each normalized_url, into its own keyword row\n",
    "    expanded_keywords_df = url_to_keyword_df['combined keywords'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "    url_to_unique_keyword_df = pd.DataFrame()\n",
    "\n",
    "    for index, row in expanded_keywords_df.iterrows():\n",
    "        row_df = row.dropna().to_frame(name='unique keyword')\n",
    "        row_df['normalized_url'] = url_to_keyword_df['normalized_url'].loc[index]   \n",
    "        url_to_unique_keyword_df = url_to_unique_keyword_df.append(row_df, ignore_index=True)\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            print(\"{} / {}\".format(index, len(expanded_keywords_df)))\n",
    "    \n",
    "    return url_to_unique_keyword_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate URL to Information Dataframe\n",
    "\n",
    "\n",
    "I don't know why this is so resource intensive...\n",
    "Maybe because of remove punctuation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.write(1, \"\\nStarting keyword generation\".encode())\n",
    "url_to_keyword_df = createUrlToKeywordDf()\n",
    "url_to_keyword_df = populateUrlToKeywordDf(url_to_keyword_df)\n",
    "\n",
    "url_to_keyword_df = addKeywordBoosting(url_to_keyword_df)\n",
    "url_lookup_cache, url_to_keyword_df, urls_without_keywords_list = generateUrlToKeywordDict(url_to_keyword_df)\n",
    "url_to_unique_keyword_df = generateKeywordToIndividualKeywordList(url_to_keyword_df)\n",
    "os.write(1, \"\\n\\tFinished keyword generation\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save URLs without any keywords\n",
    "# This is meant to be a debugging output\n",
    "\n",
    "print(\"Checking if directory (/output/) exists\")\n",
    "if not os.path.exists('./output/'):\n",
    "    print(\"Creating directory /output/\")\n",
    "    os.makedirs('./output/')\n",
    "\n",
    "os.write(1, \"Saving URLs with no keywords\".encode())\n",
    "with open('./output/URLs with NO keywords.txt', 'w', encoding='utf-8') as w:\n",
    "    for counter, url in enumerate(sorted(urls_without_keywords_list)):\n",
    "        w.write(\"{}) {}\\n\".format(counter, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter       \n",
    "\n",
    "def compute_score_with_df(user_visits_df, global_visits_counter, total_global_visits, start_date, debug=False):\n",
    "    \"\"\"\n",
    "    Description: This will take the sites that a user has visited, and perform TF-IDF calculations\n",
    "    to obtain an output score. Note that we also factor in global visits as well.\n",
    "    calculate_inverse_document_frequency\n",
    "    Input: \n",
    "    user_visits_df - This is the dataframe corresponding to an individual's activites\n",
    "    global_visits_counter - This is a Counter for all user's activites\n",
    "    \n",
    "    Output:\n",
    "    ranked_interest_df - Ranked interests. Format: Topic of Interest, Score, Corresponding URLs Visited\n",
    "    user_visits_df - The user df, but added with keywords associated with the link\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    keyword_to_logscore = calculateIdf(user_visits_df, global_visits_counter, total_global_visits, debug=False)\n",
    "    \n",
    "    columns = ['Topic of Interest', 'Score', 'Corresponding URLs Visited']\n",
    "    ranked_interest_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Iterate through all URLs the user has visited\n",
    "    for index, entry in user_visits_df.iterrows():\n",
    "        \n",
    "        url = entry['normalized_url']        \n",
    "        aggregate_keyword_list = url_lookup_cache.get(url, [])\n",
    "        \n",
    "        # Exponential Decay Factor - Calculate multiplier\n",
    "        event_date = entry['eventdate']\n",
    "        multiplier = calculateDecayMultiplier(event_date, start_date)\n",
    "        \n",
    "        # Iterate through the individual keywords extracted from the URL\n",
    "        for keyword in aggregate_keyword_list:\n",
    "            \n",
    "            if not keyword:\n",
    "                print(\"ERROR, EMPTY KEYWORD DETECTED!\")\n",
    "                print(\"URL: {}\".format(url))\n",
    "                print(\"aggregate_keyword_list: {}\".format(aggregate_keyword_list))\n",
    "\n",
    "            existing_row = ranked_interest_df[ranked_interest_df['Topic of Interest'] == keyword]\n",
    "\n",
    "            if existing_row.empty:\n",
    "                row = ranked_interest_df.shape[0]               \n",
    "                ranked_interest_df.loc[row] = [keyword, (keyword_to_logscore[keyword] * multiplier), np.NaN]\n",
    "                ranked_interest_df['Corresponding URLs Visited'] = ranked_interest_df['Corresponding URLs Visited'].astype(object)\n",
    "                ranked_interest_df.at[row, 'Corresponding URLs Visited'] = [url]\n",
    "            else:\n",
    "                                \n",
    "                index = ranked_interest_df.index[ranked_interest_df['Topic of Interest'] == keyword]\n",
    "                column = ranked_interest_df.columns.get_loc('Score')\n",
    "                updated_score = ranked_interest_df.iloc[index, column].values[0] + (keyword_to_logscore[keyword] * multiplier)\n",
    "                ranked_interest_df.iloc[index, column] = updated_score\n",
    "                \n",
    "                column = ranked_interest_df.columns.get_loc('Corresponding URLs Visited')\n",
    "                updated_urls = ranked_interest_df.iat[index.values[0], column]\n",
    "                updated_urls.append(url)                \n",
    "                ranked_interest_df.iat[index.values[0], column] = updated_urls\n",
    "\n",
    "    # Sort by logscore before returning\n",
    "    ranked_interest_df['Score'] = pd.to_numeric(ranked_interest_df['Score'])\n",
    "    ranked_interest_df.sort_values(by=['Score'], ascending=False, inplace=True)\n",
    "    \n",
    "    #\n",
    "    user_visits_df = pd.merge(user_visits_df, url_to_keyword_df, how='left', on='normalized_url', copy=True)\n",
    "    user_visits_df = user_visits_df.drop(['analyticsclient.generated_keywords', \\\n",
    "                                          'manual.keywords', \\\n",
    "                                          'analyticsclient.merged_title', \\\n",
    "                                          'analyticsclient.merged_description', \\\n",
    "                                          'analyticsclient.merged_keywords'], axis=1)\n",
    "        \n",
    "    return ranked_interest_df, user_visits_df  \n",
    "\n",
    "# TODO: Future optimiziation, only count the user visited keywords\n",
    "def calculateIdf(user_visits_df, global_keyword_counter, global_visit_count, \n",
    "                 user_weight=1.0, global_weight=2.0, \n",
    "                 debug=False, save_results=False):\n",
    "    \n",
    "    weighted_document_count = (user_visits_df.shape[0] * user_weight) + (global_visit_count * global_weight)\n",
    "    user_keyword_counter = generateCounterFromDf(user_visits_df)\n",
    "    \n",
    "    idf_lookup_dict = {}\n",
    "    \n",
    "    # Multiply by weights\n",
    "    # Note: We can \"get away\" with doing only the global weights for the user, \n",
    "    # since we're only doing TF-IDF calculations for that user.\n",
    "    for key in user_keyword_counter.keys():\n",
    "        # user_keyword_counter[key] = user_keyword_counter[key] * user_weight\n",
    "        # global_keyword_counter[key] = global_keyword_counter[key] * global_weight        \n",
    "        idf_lookup_dict[key] = math.log(weighted_document_count / ((user_keyword_counter[key] * user_weight)\n",
    "                                                                   + (global_keyword_counter[key] * global_weight)))\n",
    "        if debug or (idf_lookup_dict[key] < 0):\n",
    "            print('{} : {} [{} / ({} + {})]'.format(key, idf_lookup_dict[key], \n",
    "                                                    weighted_document_count, \n",
    "                                                    user_keyword_counter[key], global_keyword_counter[key]))\n",
    "            with pd.option_context('display.max_rows', 200, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "                display(user_visits_df)\n",
    "                \n",
    "            print(user_keyword_counter)\n",
    "            return -1\n",
    "    \n",
    "    \n",
    "    if save_results:\n",
    "        if not os.path.exists('./debug/'):\n",
    "            os.makedirs('./debug/')\n",
    "        \n",
    "        with open('./debug/IDF Dump.txt', 'w', encoding='utf-8') as w:\n",
    "            idf_df = pd.DataFrame.from_dict(idf_lookup_dict, orient='index') #, columns=['Keyword', 'IDF Value']\n",
    "            idf_df = idf_df.sort_values(0)\n",
    "    \n",
    "    return idf_lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTopicsOfInterestOnDfOfUsers(filter_grouped_user_df, global_keyword_counter, global_visit_count, start_date, debug=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        filter_grouped_user_df - This contains all the users who we're trying to calculate the topics of interest for.\n",
    "                                 This should be pre-filtered by your own specified date range.\n",
    "        global_keyword_counter - This contains the \"keyword to counts\" for the entire population, within the 30-day window\n",
    "    \n",
    "    Outputs:\n",
    "        user_to_topics_of_interest_df - This is the list of (userid, analyticskey) to (Topics of Interest, scores)\n",
    "        keyword_to_url_df - This is the user input with keyword list attached to it\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 1\n",
    "    user_to_results = dict()\n",
    "    columns = ['User ID', 'Analytics Key', 'Topic of Interest', 'Score', 'Corresponding URLs Visited']\n",
    "    user_to_topics_of_interest_df = pd.DataFrame(columns=columns)\n",
    "    all_keywords_to_url_df = pd.DataFrame()\n",
    "\n",
    "    for userid_and_analytics_key_tuple, group in filter_grouped_user_df.groupby(['userid', 'analyticskey']):\n",
    "\n",
    "        user_id = userid_and_analytics_key_tuple[0]\n",
    "        analytics_key = userid_and_analytics_key_tuple[1]\n",
    "        \n",
    "        if debug: \n",
    "            print(\"\\n{}) User ID: {} Analytics Key: {}\".format(counter, user_id, analytics_key)) \n",
    "        \n",
    "        score_df, user_with_keyword_df = compute_score_with_df(group, global_keyword_counter, global_visit_count, start_date)\n",
    "        compute_score_with_df\n",
    "        score_df['User ID'] = user_id\n",
    "        score_df['Analytics Key'] = analytics_key\n",
    "        score_df = score_df[columns]\n",
    "        user_to_topics_of_interest_df = user_to_topics_of_interest_df.append(score_df, ignore_index=True)\n",
    "\n",
    "        if debug:\n",
    "            display(user_with_keyword_df)\n",
    "        \n",
    "        all_keywords_to_url_df = all_keywords_to_url_df.append(user_with_keyword_df, ignore_index=True)\n",
    "                \n",
    "        if counter % 500 == 0:\n",
    "            print('{} / {}'.format(counter, len(filter_grouped_user_df['userid'].unique())))\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    return user_to_topics_of_interest_df, all_keywords_to_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "def extractDateRange(df, start_date, date_range='day', debug=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This takes in a dataframe, and extracts the rows where the eventdate field is within the date range specified.\n",
    "    Note that the start_date is inclusive, so if you ask for start_date = Jan 1, and range='day', you get all the \n",
    "    data from only Jan 1.\n",
    "    \"\"\"\n",
    "        \n",
    "    end_date = start_date + DATE_RANGE_OPTIONS.get(date_range, date_range)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Start Date: {}\".format(start_date))\n",
    "        print(\"Date Range: {}\".format(date_range))\n",
    "        print(\"End Date:   {}\".format(end_date))\n",
    "    \n",
    "    df = df[(df['eventdate'] > start_date) & (df['eventdate'] < end_date)].sort_values(by='eventdate', ascending=True)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Earliest Reported Date: {}\".format(df.iloc[0]['eventdate']))\n",
    "        print(\"Latest Reported Date:   {}\".format(df.iloc[-1]['eventdate']))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Testing code for function above:\n",
    "#start_date = datetime(2018, 3, 14)\n",
    "#end_date = datetime(2018, 4, 1)\n",
    "#date_range = timedelta(30)\n",
    "\n",
    "#temporary_df = extractDateRange(clean_df, start_date=start_date, date_range='week', debug=True)\n",
    "\n",
    "#display(temporary_df)\n",
    "\n",
    "\n",
    "def calculateDecayMultiplier(event_date, start_date, debug=False):\n",
    "    day_difference = (start_date - event_date).days\n",
    "    multiplier = DECAY_MULTIPLIER_BASE ** day_difference\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Start Date:   {}\".format(start_date))\n",
    "        print(\"Current Date: {}\".format(event_date))\n",
    "        print(\"Difference:   {}\".format(day_difference))\n",
    "        print(\"Multiplier:   {}\".format(multiplier))\n",
    "    \n",
    "    return multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCounterFromDf(df):\n",
    "    \"\"\"\n",
    "    This function takes in the URL history from 'normalized_url'\n",
    "    and generates the counts of each keyword as a Counter-object\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_list_of_urls = df['normalized_url'].tolist()\n",
    "    keyword_list = [url_lookup_cache.get(entry, []) for entry in list_of_list_of_urls]\n",
    "    flat_list = [entry for sublist in keyword_list for entry in sublist]\n",
    "    keyword_counter = Counter(flat_list)\n",
    "    \n",
    "    return keyword_counter\n",
    "\n",
    "\n",
    "def calculateInfoForAllIndividualUsers(user_df, global_df, start_date, end_date, time_period='day', debug=False):\n",
    "    \"\"\"\n",
    "    This function will iterate through all the users from user_df, and return all the individual's scores\n",
    "    \"\"\"\n",
    "    \n",
    "    current_date = start_date\n",
    "    all_users_to_topic_of_interest_df = pd.DataFrame()\n",
    "    \n",
    "    # TODO: PERFORMANCE BOOST IDEA: Limit the date range for the user_df and global_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    while current_date < end_date:\n",
    "        print(\"current_date: {}\".format(current_date))\n",
    "        current_execution_time = datetime.now()\n",
    "        \n",
    "        # We want to look 30-days back for calcuations\n",
    "        date_range_filtered_user_df = extractDateRange(user_df, \n",
    "                                                       start_date=(current_date - INTEREST_CALCULATION_WINDOW_TIMEDELTA), \n",
    "                                                       date_range=(INTEREST_CALCULATION_WINDOW_TIMEDELTA + timedelta(1)), \n",
    "                                                       debug=False)\n",
    "        date_range_filtered_global_df = extractDateRange(global_df, \n",
    "                                                         start_date=(current_date - INTEREST_CALCULATION_WINDOW_TIMEDELTA), \n",
    "                                                         date_range=(INTEREST_CALCULATION_WINDOW_TIMEDELTA + timedelta(1)), \n",
    "                                                         debug=False)\n",
    "               \n",
    "        # Obtain number of visits by all users\n",
    "        global_visit_counter = date_range_filtered_global_df.shape[0]\n",
    "        \n",
    "        # Generate counts for global_df\n",
    "        global_keyword_counter = generateCounterFromDf(date_range_filtered_global_df)\n",
    "        \n",
    "        # do Interest calculations for individuals\n",
    "        user_to_topics_of_interest_df, user_keyword_subset_df = calculateTopicsOfInterestOnDfOfUsers(date_range_filtered_user_df, \n",
    "                                                                                                     global_keyword_counter,\n",
    "                                                                                                     global_visit_counter,\n",
    "                                                                                                     (current_date + timedelta(1)))        \n",
    "        user_to_topics_of_interest_df['currdate'] = current_date\n",
    "\n",
    "        # append to larger list\n",
    "        all_users_to_topic_of_interest_df = all_users_to_topic_of_interest_df.append(user_to_topics_of_interest_df, ignore_index=True)\n",
    "\n",
    "        current_date += timedelta(1)\n",
    "        \n",
    "        total_execution_time_for_loop = datetime.now() - current_execution_time\n",
    "        print(\"Time Elapsed: {}\".format(total_execution_time_for_loop))\n",
    "        \n",
    "    return all_users_to_topic_of_interest_df\n",
    "\n",
    "def calculateInfoForAllIndividualUsersSaveToJSON(user_to_toi_and_score, save_directory, debug=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    partition_key = datetime.today().strftime('%Y%m%d0000')\n",
    "    \n",
    "    # Create Output Directory if it doesn't already exist\n",
    "    if not os.path.exists(save_directory) and not LCS_MESSAGING_ENABLED:\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Gameplan:\n",
    "    # - Go through date/userid/analyticskey\n",
    "    # - Go through each keyword & score\n",
    "    # - Find all URLs & Counts that correspond to the keyword\n",
    "    # - Save info as a JSON entry\n",
    "    \n",
    "    for curr_date, row in user_to_toi_and_score.groupby(['currdate']):\n",
    "\n",
    "        if not LCS_MESSAGING_ENABLED:\n",
    "            full_directory_and_file_name = os.path.join(save_directory, curr_date.strftime('%Y%m%d') + '.json')\n",
    "            output_file = open(full_directory_and_file_name, 'w', encoding='utf-8')\n",
    "        \n",
    "        partition_key = datetime.today().strftime('%Y%m%d0000')\n",
    "        curr_date_string = curr_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        counter = 1\n",
    "        current_execution_time = datetime.now()\n",
    "        print(\"currdate: {}\".format(curr_date_string))\n",
    "        \n",
    "        for userid_and_analytics_key, row2 in row.groupby(['User ID', 'Analytics Key']):\n",
    "            user_id = userid_and_analytics_key[0]\n",
    "            analytics_key = userid_and_analytics_key[1]\n",
    "            \n",
    "            if debug:\n",
    "                print(\"User ID: {}\".format(user_id))\n",
    "                print(\"Analytics Key: {}\".format(analytics_key))\n",
    "                \n",
    "            row2 = row2.sort_values(by=['Score'], ascending=False)\n",
    "            \n",
    "            user_to_keyword_info_list = []\n",
    "                \n",
    "            for toi_score, row3 in row2.groupby(['Topic of Interest', 'Score']):\n",
    "                topic_of_interest = toi_score[0]\n",
    "                score = toi_score[1]\n",
    "\n",
    "                if debug:\n",
    "                    print(\"\\t{}: {}\".format(topic_of_interest, score))\n",
    "                \n",
    "                # Generates [URL, View Count]\n",
    "                url_to_view_count_df = row3['Corresponding URLs Visited'].apply(lambda x: pd.Series(x).value_counts()).T.reset_index()\n",
    "                url_to_view_count_df.rename(columns={url_to_view_count_df.columns[0] : 'url', url_to_view_count_df.columns[1] : 'visitCount'}, inplace=True)\n",
    "                \n",
    "                if debug:\n",
    "                    display(url_to_view_count_df)\n",
    "                \n",
    "                url_to_visit_count_list = []\n",
    "                \n",
    "                for index, url_visit_count in url_to_view_count_df.iterrows():\n",
    "                    url = url_visit_count['url']\n",
    "                    visit_count = url_visit_count['visitCount']\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(\"URL: {}\".format(url))\n",
    "                        print(\"visitCount: {}\".format(visit_count))\n",
    "                        \n",
    "                    url_to_visit_count_list.append(\n",
    "                        OrderedDict([('url', url), \n",
    "                                     ('uniqueVisitsCount', visit_count)]))\n",
    "\n",
    "\n",
    "                user_to_keyword_info_list.append(\n",
    "                    OrderedDict([('name', topic_of_interest),\n",
    "                                 ('score', score), \n",
    "                                 ('pagesVisited', url_to_visit_count_list)]))\n",
    "\n",
    "            json_text = json.dumps(\n",
    "                OrderedDict([('analyticsKey', analytics_key), \n",
    "                            ('partitionkey', partition_key),\n",
    "                            ('userId', user_id),\n",
    "                            ('currdate', curr_date_string),\n",
    "                            ('interests', user_to_keyword_info_list)]))\n",
    "\n",
    "            if not LCS_MESSAGING_ENABLED:\n",
    "                output_file.write(\"{}\\n\".format(json_text))\n",
    "            else:\n",
    "                lcsMessageBusService.sendMessage(INDIVIDUAL_LCS_DESTINATION_NAME, json_text)\n",
    "            counter += 1\n",
    "            \n",
    "            if counter % 1000 == 0:\n",
    "                print(\"{} / {}\".format(counter, len(row2)))\n",
    "                \n",
    "        total_execution_time_for_loop = datetime.now() - current_execution_time\n",
    "        print(\"Time Elapsed: {}\".format(total_execution_time_for_loop))\n",
    "        \n",
    "        if not LCS_MESSAGING_ENABLED:\n",
    "            output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInfoForAllSegmentsSaveToJSON(segment_to_toi_and_score_df, user_to_toi_and_score, score_threshold, save_directory):\n",
    "    \n",
    "    partition_key = datetime.today().strftime('%Y%m%d0000')\n",
    "    \n",
    "    # Create Output Directory if it doesn't already exist\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    for curr_date, row in segment_to_toi_and_score_df.groupby(['currdate']):\n",
    "        curr_date_string = curr_date.strftime(\"%Y-%m-%d\")\n",
    "        print(\"currdate: {}\".format(curr_date_string))\n",
    "        \n",
    "        if not LCS_MESSAGING_ENABLED:\n",
    "            full_directory_and_file_name = os.path.join(save_directory, curr_date.strftime('%Y%m%d') + '.json')\n",
    "            output_file = open(full_directory_and_file_name, 'w', encoding='utf-8')\n",
    "        \n",
    "        user_to_toi_and_score_filtered_by_date_df = user_to_toi_and_score[user_to_toi_and_score['currdate'] == curr_date]\n",
    "        \n",
    "        for segment_id, row2 in row.groupby(['segmentIdentifier']):\n",
    "            print(\"\\tsegmentIdentifier: {}\".format(segment_id))\n",
    "            user_to_toi_and_score_filtered_by_date_and_segment_id_df = getSegmentEntriesDf(user_to_toi_and_score_filtered_by_date_df, segment_id)\n",
    "            #display(user_to_toi_and_score_filtered_by_date_and_segment_id_df)\n",
    "\n",
    "            # Create veritcal list of [topic of interest, score]\n",
    "            keyword_to_score_df = row2.drop(labels=['currdate', 'segmentIdentifier'], axis=1, inplace=False).T.reset_index().copy()\n",
    "            keyword_to_score_df.columns.values[0] = 'Topic of Interest'\n",
    "            keyword_to_score_df.columns.values[1] = 'Score'\n",
    "\n",
    "            #with pd.option_context('display.max_rows', 1000, 'display.max_columns', None, 'display.max_colwidth', 2000):\n",
    "            #    display(keyword_to_score_df)\n",
    "                \n",
    "            keyword_to_url_json_string_list = []\n",
    "\n",
    "            # Iterate through the current date + segment users, to figure out corresponding URLs\n",
    "            for index, row3 in keyword_to_score_df.iterrows():\n",
    "                topic_of_interest = row3['Topic of Interest']\n",
    "                score = row3['Score']\n",
    "\n",
    "                \n",
    "\n",
    "                # Skip NaN values\n",
    "                if math.isnan(score):\n",
    "                    #print(\"Skipping...\")\n",
    "                    continue\n",
    "                    \n",
    "                print(\"\\t\\t{} : {}\".format(topic_of_interest, score))\n",
    "\n",
    "                # Find corresponding users whose individual scores exceed the threshold\n",
    "                # Need URL, uniqueVisitsCount\n",
    "                url_to_counts = getUrlAndUniqueVisitsCount(user_to_toi_and_score_filtered_by_date_and_segment_id_df, topic_of_interest, score_threshold)\n",
    "                \n",
    "                # If it's an emtpy list\n",
    "                if url_to_counts.empty:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                #display(url_to_counts)\n",
    "                url_to_view_count_list = []\n",
    "    \n",
    "                for url, view_count in url_to_counts.items():\n",
    "                    url_to_view_count_entry = OrderedDict([('url', url),\n",
    "                                                           ('uniqueVisitsCount', view_count)])\n",
    "                    url_to_view_count_list.append(url_to_view_count_entry)\n",
    "                    \n",
    "                keyword_entry = OrderedDict([('name', topic_of_interest), \n",
    "                                             ('score', score), \n",
    "                                             ('pagesVisited', url_to_view_count_list)])\n",
    "                keyword_to_url_json_string_list.append(keyword_entry)\n",
    "\n",
    "            json_text = json.dumps(OrderedDict([('partitionKey', partition_key),\n",
    "                                                ('segmentIdentifier', segment_id),\n",
    "                                                ('analyticsKey', 'www.liferay.com'), \n",
    "                                                ('currdate', curr_date_string), \n",
    "                                                ('interests', keyword_to_url_json_string_list)]))\n",
    "            if not LCS_MESSAGING_ENABLED:\n",
    "                output_file.write(\"{}\\n\".format(json_text))\n",
    "            else:\n",
    "                lcsMessageBusService.sendMessage(SEGMENT_LCS_DESTINATION_NAME, json_text)\n",
    "            \n",
    "        if not LCS_MESSAGING_ENABLED:\n",
    "            output_file.close()\n",
    "    \n",
    "def getSegmentEntriesDf(df, segmentIdentifier):\n",
    "    \"\"\"\n",
    "    This will only return rows that match the segmentIdentifier\n",
    "    \"\"\"\n",
    "    \n",
    "    only_segment_id_entries_df = pd.merge(segment_lookup_df, df, how='inner', left_on='datasourceindividualpk', right_on='User ID', sort=True)#.drop('value', 1)\n",
    "    #display(only_segment_id_entries_df)\n",
    "    \n",
    "    return only_segment_id_entries_df\n",
    "    \n",
    "    \n",
    "def getUrlAndUniqueVisitsCount(df, topic_of_interest, minimum_score_threshold, debug=False):\n",
    "    \n",
    "    url_to_unique_visits = OrderedDict()\n",
    "    \n",
    "    toi_df = df[(df['Topic of Interest'] == topic_of_interest) \n",
    "                & (df['Score'] >= minimum_score_threshold)]\n",
    "    \n",
    "    if toi_df.empty:\n",
    "        return toi_df\n",
    "    \n",
    "    expanded_url_list = toi_df.set_index(['User ID'])['Corresponding URLs Visited'].apply(pd.Series).stack()\n",
    "    expanded_url_list = pd.DataFrame(expanded_url_list).reset_index().drop(labels=['level_1'], axis=1, inplace=False)\n",
    "    expanded_url_list.rename(columns={0 : 'Corresponding URLs Visited'}, inplace=True)\n",
    "    # We are only getting unique: (userid, url) pairs\n",
    "    no_duplicates_df = expanded_url_list.drop_duplicates(subset=['User ID', 'Corresponding URLs Visited'])\n",
    "    count_url_visits = no_duplicates_df['Corresponding URLs Visited'].value_counts()\n",
    "    \n",
    "    if debug:\n",
    "        display(toi_df)\n",
    "        display(expanded_url_list)\n",
    "        display(no_duplicates_df)\n",
    "        display(count_url_visits)\n",
    "\n",
    "    for url_count_tuple in count_url_visits.iteritems():\n",
    "        url = url_count_tuple[0]\n",
    "        count = url_count_tuple[1]\n",
    "        url_to_unique_visits[url] = count\n",
    "    \n",
    "    return count_url_visits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSegmentWithDf(user_to_topic_of_interest_df, MINIMUM_SCORE_THRESHOLD):    \n",
    "    user_to_toi_filtered_by_minimum_score = user_to_topic_of_interest_df[user_to_topic_of_interest_df['Score'] > MINIMUM_SCORE_THRESHOLD]\n",
    "    keyword_to_count = user_to_topic_of_interest_df.groupby('Topic of Interest').count()\n",
    "    keyword_to_count['Logscore'] = keyword_to_count['User ID'].apply(lambda x: math.log1p(x))\n",
    "    keyword_to_count = keyword_to_count[['Logscore']]\n",
    "\n",
    "    return keyword_to_count  \n",
    "\n",
    "def calculateSegmentInfoFromIndividualDf(segment_name, user_to_toi_df, score_threshold, debug=False):\n",
    "    \"\"\"\n",
    "    This will calculate the interest scores, and\n",
    "    \"\"\"\n",
    "    \n",
    "    user_to_toi_with_date_df = pd.DataFrame()\n",
    "    \n",
    "    # Filter by date\n",
    "    for index, row in user_to_toi_df.groupby('currdate'):\n",
    "        \n",
    "        if debug:\n",
    "            print(\"currdate: {}\".format(index))\n",
    "            display(row)\n",
    "            \n",
    "        segment_to_topic_of_interest_df = calculateSegmentWithDf(row, score_threshold)\n",
    "        segment_to_topic_of_interest_transposed_df = segment_to_topic_of_interest_df.T\n",
    "        segment_to_topic_of_interest_transposed_df['currdate'] = index\n",
    "        user_to_toi_with_date_df = user_to_toi_with_date_df.append(segment_to_topic_of_interest_transposed_df, ignore_index=True)\n",
    "        \n",
    "        if debug:\n",
    "            display(user_to_toi_with_date_df)\n",
    "\n",
    "    # Move currdate column to front\n",
    "    currdate_column = user_to_toi_with_date_df['currdate']\n",
    "    user_to_toi_with_date_df.drop('currdate', axis=1, inplace=True)\n",
    "    user_to_toi_with_date_df.insert(0, 'currdate', currdate_column)\n",
    "    \n",
    "    # Add Segment Name column\n",
    "    user_to_toi_with_date_df.insert(1, 'segmentIdentifier', segment_name)\n",
    "    \n",
    "    return user_to_toi_with_date_df\n",
    "\n",
    "def calculateAllSegmentInfo(user_to_toi_df, debug=False):\n",
    "    \"\"\"\n",
    "    This function will return a DataFrame of all segments Topic of Interests and Scores\n",
    "    \"\"\"\n",
    "    \n",
    "    all_segment_info_df = pd.DataFrame()\n",
    "    \n",
    "    # Gameplan:\n",
    "    # - Iterate through the list of segments\n",
    "    #   * Filter user_to_toi_df so we only get the users from that segment\n",
    "    # - Calculate segment toi & scores for that segment\n",
    "    for segmentName, row in segment_lookup_df.groupby('segmentName'):\n",
    "        display(\"segmentName: {}\".format(segmentName))\n",
    "        filtered_user_df = pd.merge(row, user_to_toi_df, how='inner', left_on='datasourceindividualpk', right_on='User ID')\n",
    "        print(\"\\tMembers: {}\".format(filtered_user_df['User ID'].unique()))\n",
    "        if debug:\n",
    "            display(filtered_user_df)\n",
    "            \n",
    "        if filtered_user_df.shape[0] == 0:\n",
    "            print(\"[WARNING] - Segment has 0 users! Skipping...\")\n",
    "            continue\n",
    "\n",
    "        segment_toi_to_score_df = calculateSegmentInfoFromIndividualDf(segmentName, filtered_user_df, MINIMUM_TOPIC_OF_INTEREST_THRESHOLD_SCORE)\n",
    "        all_segment_info_df = all_segment_info_df.append(segment_toi_to_score_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Move currdate & segmentIdentifier to front\n",
    "    currdate_column = all_segment_info_df['currdate']\n",
    "    segment_id_column = all_segment_info_df['segmentIdentifier']\n",
    "    all_segment_info_df.drop('currdate', axis=1, inplace=True)\n",
    "    all_segment_info_df.drop('segmentIdentifier', axis=1, inplace=True)\n",
    "    all_segment_info_df.insert(0, 'currdate', currdate_column)\n",
    "    all_segment_info_df.insert(1, 'segmentIdentifier', segment_id_column)\n",
    "    \n",
    "    return all_segment_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printUsefulUserDfInformation(df):\n",
    "    print(\"Rows:         {}\".format(df.shape[0]))\n",
    "    print(\"Unique Users: {}\".format(len(df.groupby('userid'))))\n",
    "    print(\"Unique URLs:  {}\".format(len(df.groupby('normalized_url'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with Output saved as JSON files\n",
    "\n",
    "Steps:\n",
    "* Filter out the group of users you want as a dataframe\n",
    "* Pass in date range for calculations\n",
    "* Write output files\n",
    " * Individual -> Topic of Interest (individual topics of interest.json)\n",
    " * Entire Segment -> Topic of Interest (segment topics of interest.json)\n",
    " * Segment URLs Contribution -> Topic of Interest (daily URL contribution to topics of interest.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MINIMUM_VIEWING_TIME_CONSIDERED = 10000\n",
    "\n",
    "start_date = START_DATE_DATETIME\n",
    "end_date = END_DATE_DATETIME\n",
    "#start_date = datetime(2018, 4, 1)\n",
    "#end_date = datetime(2018, 5, 1)\n",
    "\n",
    "printUsefulUserDfInformation(clean_df)\n",
    "\n",
    "print(\"\\nFiltering out Ignore URLs\")\n",
    "global_df = clean_df[(~clean_df['Ignore URL'])]\n",
    "printUsefulUserDfInformation(global_df)\n",
    "\n",
    "print(\"\\nFiltering out views not exceeding {}ms\".format(MINIMUM_VIEWING_TIME_CONSIDERED))\n",
    "global_df = global_df[global_df['eventproperties.viewDuration'] >= MINIMUM_VIEWING_TIME_CONSIDERED]\n",
    "printUsefulUserDfInformation(global_df)\n",
    "\n",
    "print(\"\\nFiltering out views not within time range\")\n",
    "print((start_date - INTEREST_CALCULATION_WINDOW_TIMEDELTA))\n",
    "print(end_date + timedelta(days=1))\n",
    "global_df = global_df[(global_df['eventdate'] >= (start_date - INTEREST_CALCULATION_WINDOW_TIMEDELTA))\n",
    "                  & (global_df['eventdate'] <= (end_date + timedelta(days=1)))]\n",
    "printUsefulUserDfInformation(global_df)\n",
    "\n",
    "global_df = global_df.sort_values(by='eventdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter \n",
    "printUsefulUserDfInformation(global_df)\n",
    "user_df = global_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Filtering out users who did not have at least 5 unique views\")\n",
    "user_df = user_df.groupby('userid').filter(lambda x: len(x) > 5)\n",
    "printUsefulUserDfInformation(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None, 'display.max_colwidth', 50):\n",
    "    display(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Calculating all Individual User's Info\")\n",
    "os.write(1, \"\\nCalculating all Individual User's Info\".encode())\n",
    "user_to_toi_and_score = calculateInfoForAllIndividualUsers(user_df, global_df, (start_date + timedelta(30)), (start_date + timedelta(37)), 'day', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Saving Individual Info to JSON file\")\n",
    "os.write(1, \"\\nSaving Individual Info to JSON file\".encode())\n",
    "calculateInfoForAllIndividualUsersSaveToJSON(user_to_toi_and_score, INDIVIDUAL_OUTPUT_DIRECTORY, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Calculating all Segment Info\")\n",
    "os.write(1, \"\\nCalculating all Segment Info\".encode())\n",
    "all_segment_info_df = calculateAllSegmentInfo(user_to_toi_and_score, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Saving Segment Info to JSON file\")\n",
    "os.write(1, \"\\nSaving Segment Info to JSON file\".encode())\n",
    "calculateInfoForAllSegmentsSaveToJSON(all_segment_info_df, \n",
    "                                      user_to_toi_and_score, \n",
    "                                      0, #MINIMUM_TOPIC_OF_INTEREST_THRESHOLD_SCORE,\n",
    "                                      SEGMENT_OUTPUT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.write(1, \"\\n\\nEverything is finished!\".encode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
